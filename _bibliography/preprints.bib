@misc{ray2024swiftqueueoptimizinglowlatencyapplications,
      title={{S}wift{Q}ueue: {O}ptimizing {L}ow-{L}atency {A}pplications with {S}wift {P}acket {Q}ueuing}, 
      author={Siddhant Ray and Xi Jiang and Jack Luo and Nick Feamster and Junchen Jiang},
      year={2024},
      eprint={2410.06112},
      archivePrefix={arXiv},
      primaryClass={cs.NI},
      note={In Submission},
      url={https://arxiv.org/abs/2410.06112}, 
      abbr={arXiv},
      abstract={Low Latency, Low Loss, and Scalable Throughput (L4S), as an emerging router-queue management technique, has seen steady deployment in the industry. An L4S-enabled router assigns each packet to the queue based on the packet header marking. Currently, L4S employs per-flow queue selection, i.e. all packets of a flow are marked the same way and thus use the same queues, even though each packet is marked separately. However, this may hurt tail latency and latency-sensitive applications because transient congestion and queue buildups may only affect a fraction of packets in a flow.
                We present SwiftQueue, a new L4S queue-selection strategy in which a sender uses a novel per-packet latency predictor to pinpoint which packets likely have latency spikes or drops. The insight is that many packet-level latency variations result from complex interactions among recent packets at shared router queues. Yet, these intricate packet-level latency patterns are hard to learn efficiently by traditional models. Instead, SwiftQueue uses a custom Transformer, which is well-studied for its expressiveness on sequential patterns, to predict the next packet's latency based on the latencies of recently received ACKs. Based on the predicted latency of each outgoing packet, SwiftQueue's sender dynamically marks the L4S packet header to assign packets to potentially different queues, even within the same flow. Using real network traces, we show that SwiftQueue is 45-65% more accurate in predicting latency and its variations than state-of-art methods. Based on its latency prediction, SwiftQueue reduces the tail latency for L4S-enabled flows by 36-45%, compared with the existing L4S queue-selection method.},
    bibtex_show={true},
    arxiv={2410.06112},
}

@misc{ray2024ragservefastqualityawarerag,
      title={{RAGS}erve: {F}ast {Q}uality-{A}ware {RAG} {S}ystems with {C}onfiguration {A}daptation}, 
      author={Siddhant Ray and Rui Pan and Zhuohan Gu and Kuntai Du and Ganesh Ananthanarayanan and Ravi Netravali and Junchen Jiang},
      year={2024},
      eprint={2412.10543},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2412.10543}, 
      bibtex_show={true},
      abbr={arXiv},
      arxiv={2412.10543},
      abstract={RAG (Retrieval Augmented Generation) allows LLMs (large language models) to generate better responses with external knowledge, but using more external knowledge often improves 
          generation quality at the expense of response delay. Prior work either reduces the response delay (through better scheduling of RAG queries) or strives to maximize quality (which involves tuning the RAG workflow),
          but they fall short in optimizing the tradeoff between the delay and quality of RAG responses.
          This paper presents RAGServe, the first RAG system that jointly schedules queries and adapts 
          the key RAG configurations of each query, such as the number of retrieved text chunks and 
          synthesis methods, in order to balance quality optimization and response delay reduction. 
          Using 4 popular RAG-QA datasets, we show that compared with the state-of-the-art RAG optimization 
          schemes, RAGServe reduces the generation latency by 1.64-2.54X without sacrificing generation quality.},
}
