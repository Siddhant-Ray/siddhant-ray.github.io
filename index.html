<!DOCTYPE html> <html lang="en"> <head> <meta name="google-site-verification" content=""/> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Siddhant Ray</title> <meta name="author" content="Siddhant Ray"/> <meta name="description" content="Siddhant Ray's academic website. "/> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <meta property="og:site_name" content="Siddhant Ray"/> <meta property="og:type" content="website"/> <meta property="og:title" content="Siddhant Ray | About"/> <meta property="og:url" content="https://siddhant-ray.github.io/"/> <meta property="og:description" content="Siddhant Ray's academic website. "/> <meta property="og:locale" content="en"/> <meta name="twitter:card" content="summary"/> <meta name="twitter:title" content="About"/> <meta name="twitter:description" content="Siddhant Ray's academic website. "/> <meta name="twitter:site" content="@siddhantrayyy"/> <meta name="twitter:creator" content="@siddhantrayyy"/> <script type="application/ld+json">
      {
        "author":
        {
          "@type": "Person",
          "name": "Siddhant  Ray"
        },
        "url": "https://siddhant-ray.github.io/",
        "@type": "WebSite",
        "description": "Siddhant Ray's academic website.
",
        "headline": "About",
        "sameAs": ["https://scholar.google.com/citations?user=sHQFKFUAAAAJ", "https://github.com/Siddhant-Ray", "https://www.linkedin.com/in/siddhant-ray", "https://twitter.com/siddhantrayyy"],
        "name": "Siddhant  Ray",
        "@context": "https://schema.org"
      }
    </script> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>‚ôæÔ∏è</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://siddhant-ray.github.io/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Siddhant</span> Ray </h1> <p class="desc"></p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/newpp-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/newpp-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/newpp-1400.webp"></source> <img src="/assets/img/newpp.jpg" class="img-fluid z-depth-1 rounded-circle" width="auto" height="auto" alt="newpp.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="address"> </div> </div> <div class="clearfix"> <p align="justify"> I am a second year PhD student in Computer Science at the <a href="https://cs.uchicago.edu/academics/phd/" target="_blank" rel="noopener noreferrer"> University of Chicago</a>, advised by <a href="https://people.cs.uchicago.edu/~junchenj/" target="_blank" rel="noopener noreferrer"> Junchen Jiang</a> and <a href="https://people.cs.uchicago.edu/~feamster/" target="_blank" rel="noopener noreferrer"> Nick Feamster</a>. I am interested in machine learning methods for performance improvement in computer networks and efficient serving systems for Large Language Models. </p> <p align="justify"> Currently I work on <i> joint optimizations </i> in Retrieval-Augmented-Generation (RAG) systems on quality and delay with <i> query level configuration selection and resource scheduling </i>. I also work on using <i> Transformer models </i> for per-packet latency prediction to <i> improve queue selection and reduce tail-latency </i> for latency sensitive applications. </p> <p align="justify"> In the past, I have worked on advances in Software Defined Networking, programmable networks and cloud computing. Additionally I have spent some time working on developing NLP techniques to analyse political corpora. </p> <p align="justify"> I'm fortunate to be additionally supported by the <a href="https://news.uchicago.edu/story/205-million-gifts-support-expansion-initiatives-computer-science" target="_blank" rel="noopener noreferrer"> Liew Family Graduate Fellowship</a>. Prior to starting my PhD, I earned my MSc in Electrical Engineering and Information Technology at <a href="https://ee.ethz.ch/studies/master-s-programmes/main-master.html" target="_blank" rel="noopener noreferrer"> ETH Zurich</a> and my B.Tech in Electronics and Communication Engineering at <a href="https://vit.ac.in/schools/school-of-electronics-engineering" target="_blank" rel="noopener noreferrer">VIT Vellore</a>. </p> </div> <div class="news"> <h2>News</h2> <div class="table-responsive" style="max-height: 18vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row">Sep, 2025</th> <td> Serving as a Reviewer for <a href="https://aaai.org/conference/aaai/aaai-26/" target="_blank" rel="noopener noreferrer"> AAAI‚Äô26 </a>. </td> </tr> <tr> <th scope="row">Jun, 2025</th> <td> Starting my research internship at Microsoft, Redmond jointly working with Microsoft Research and Outlook. </td> </tr> <tr> <th scope="row">May, 2025</th> <td> Selected and awarded travel grant for the inaugural <a href="https://ldos.utexas.edu/events/researchschool" target="_blank" rel="noopener noreferrer"> PhD Research School </a> held by the <a href="https://ldos.utexas.edu/" target="_blank" rel="noopener noreferrer"> LDOS expedition </a> at UT Ausin. </td> </tr> <tr> <th scope="row">May, 2025</th> <td> Serving as on the Artifact Evaluation Committee for <a href="https://www.usenix.org/conference/atc25#organizers" target="_blank" rel="noopener noreferrer"> USENIX ATC‚Äô25 </a>, <a href="https://www.usenix.org/conference/osdi25#organizers" target="_blank" rel="noopener noreferrer"> OSDI‚Äô25 </a> and <a href="https://conferences.sigcomm.org/co-next/2025/#!/artifact-committee" target="_blank" rel="noopener noreferrer"> CoNEXT‚Äô25 </a>. </td> </tr> <tr> <th scope="row">Jan, 2025</th> <td> Serving as a Reviewer for <a href="https://icml.cc/Conferences/2025/ProgramCommittee" target="_blank" rel="noopener noreferrer"> ICML‚Äô25 </a>. </td> </tr> </table> </div> </div> <div class="publications"> <h2>Selected publications</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#d87c5a"><a href="https://dl.acm.org/doi/proceedings/10.1145/3689031" target="_blank" rel="noopener noreferrer">EuroSys</a></abbr></div> <div id="10.1145/3689031.3696098" class="col-sm-8"> <div class="title">CacheBlend: Fast Large Language Model Serving for RAG with Cached Knowledge Fusion</div> <div class="author"> Jiayi Yao,¬†Hanchen Li,¬†Yuhan Liu,¬†<em>Siddhant Ray</em>,¬†Yihua Cheng,¬†Qizheng Zhang,¬†Kuntai Du,¬†Shan Lu,¬†and¬†Junchen Jiang</div> <div class="periodical"> <em>In Proceedings of the Twentieth European Conference on Computer Systems</em> 2025 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/https://arxiv.org/abs/2405.16444" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://2025.eurosys.org/awards.html" class="btn btn-sm z-depth-0" style="color: green; background-color: white; font-weight: bold; font-family: 'Open Sans', sans-serif; border: none;font-size: 1.18em; position: relative; left: -0.25cm; padding: 2px 6px;" role="button" target="_blank" rel="noopener noreferrer"> <span style="font-size: 1.2em;">üèÖ</span> Best Paper</a> </div> <div class="abstract hidden"> <p>Large language models (LLMs) often incorporate multiple text chunks in their inputs to provide the necessary contexts. To speed up the prefill of the long LLM inputs, one can pre-compute the KV cache of a text and re-use the KV cache when the context is reused as the prefix of another LLM input. However, the reused text chunks are not always the input prefix, which makes precomputed KV caches not directly usable since they ignore the text‚Äôs cross-attention with the preceding texts. Thus, the benefits of reusing KV caches remain largely unrealized.This paper tackles just one challenge: when an LLM input contains multiple text chunks, how to quickly combine their precomputed KV caches in order to achieve the same generation quality as the expensive full prefill (i.e., without reusing KV cache)? This challenge naturally arises in retrieval-augmented generation (RAG) where the input is supplemented with multiple retrieved texts as the context. We present CacheBlend, a scheme that reuses the precomputed KV caches, regardless prefix or not, and selectively recomputes the KV values of a small subset of tokens to partially update each reused KV cache. In the meantime, the small extra delay for recomputing some tokens can be pipelined with the retrieval of KV caches within the same job, allowing CacheBlend to store KV caches in slower devices with more storage capacity while retrieving them without increasing the inference delay. By comparing CacheBlend with the state-of-the-art KV cache reusing schemes on three open-source LLMs of various sizes and four popular benchmark datasets of different tasks, we show that CacheBlend reduces time-to-first-token (TTFT) by 2.2-3.3\texttimes and increases the inference throughput by 2.8-5\texttimes from full KV recompute without compromising generation quality. The code is available at https://github.com/LMCache/LMCache.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">10.1145/3689031.3696098</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yao, Jiayi and Li, Hanchen and Liu, Yuhan and Ray, Siddhant and Cheng, Yihua and Zhang, Qizheng and Du, Kuntai and Lu, Shan and Jiang, Junchen}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{C}ache{B}lend: {F}ast {L}arge {L}anguage {M}odel {S}erving for {RAG} with {C}ached {K}nowledge {F}usion}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9798400711961}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3689031.3696098}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3689031.3696098}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Twentieth European Conference on Computer Systems}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{94‚Äì109}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{16}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{KV Cache, Large Language Models, Retrieval-Augmented-Generation}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Rotterdam, Netherlands}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{EuroSys '25}</span><span class="p">,</span>
  <span class="na">award_name</span> <span class="p">=</span> <span class="s">{Best Paper}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#d87c5a"><a href="https://conferences.sigcomm.org/sigcomm/2024/" target="_blank" rel="noopener noreferrer">SIGCOMM</a></abbr></div> <div id="10.1145/3651890.3672274" class="col-sm-8"> <div class="title">CacheGen: KV Cache Compression and Streaming for Fast Large Language Model Serving</div> <div class="author"> Yuhan Liu,¬†Hanchen Li,¬†Yihua Cheng,¬†<em>Siddhant Ray</em>,¬†Yuyang Huang,¬†Qizheng Zhang,¬†Kuntai Du,¬†Jiayi Yao,¬†Shan Lu,¬†Ganesh Ananthanarayanan,¬†Michael Maire,¬†Henry Hoffmann,¬†Ari Holtzman,¬†and¬†Junchen Jiang</div> <div class="periodical"> <em>In Proceedings of the ACM SIGCOMM 2024 Conference</em> 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2310.07240" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>s large language models (LLMs) take on complex tasks, their inputs are supplemented with longer contexts that incorporate domain knowledge. Yet using long contexts is challenging, as nothing can be generated until the whole context is processed by the LLM. While the context-processing delay can be reduced by reusing the KV cache of a context across different inputs, fetching the KV cache, which contains large tensors, over the network can cause high extra network delays. CacheGen is a fast context-loading module for LLM systems. First, CacheGen uses a custom tensor encoder, leveraging KV cache‚Äôs distributional properties to encode a KV cache into more compact bitstream representations with negligible decoding overhead, to save bandwidth usage. Second, CacheGen adapts the compression level of different parts of a KV cache to cope with changes in available bandwidth, in order to maintain low context-loading delay and high generation quality. % When available bandwidth drops, CacheGen may raise the compression level for a part of the context or recompute its KV cache on the fly. We test CacheGen on popular LLMs and datasets. Compared to the recent systems that reuse the KV cache, CacheGen reduces the KV cache size by 3.5-4.3x and the total delay in fetching and processing contexts by 3.2-3.7x with negligible impact on the LLM response quality. Our code is at: this https URL.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">10.1145/3651890.3672274</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Liu, Yuhan and Li, Hanchen and Cheng, Yihua and Ray, Siddhant and Huang, Yuyang and Zhang, Qizheng and Du, Kuntai and Yao, Jiayi and Lu, Shan and Ananthanarayanan, Ganesh and Maire, Michael and Hoffmann, Henry and Holtzman, Ari and Jiang, Junchen}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Cache{G}en: KV {C}ache {C}ompression and {S}treaming for {F}ast {L}arge {L}anguage {M}odel {S}erving}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9798400706141}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{ACM}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3651890.3672274}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3651890.3672274}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the ACM SIGCOMM 2024 Conference}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{large language models, KV cache, compression}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Sydney, NSW, Australia}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#d87c5a"><a href="https://conferences.sigcomm.org/hotnets/2022/index.html" target="_blank" rel="noopener noreferrer">HOTNETS</a></abbr></div> <div id="dietmuller2022new" class="col-sm-8"> <div class="title">A New Hope for Network Model Generalization</div> <div class="author"> Alexander Dietm√ºller,¬†<em>Siddhant Ray</em>,¬†Romain Jacob,¬†and¬†Laurent Vanbever</div> <div class="periodical"> <em>In Proceedings of the 21st ACM Workshop on Hot Topics in Networks</em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2207.05843" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/Siddhant-Ray/Network-Traffic-Transformer/tree/ACM-HotNets" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Generalizing machine learning (ML) models for network traffic dynamics tends to be considered a lost cause. Hence, for every new task, we often resolve to design new models and train them on model-specific datasets collected, whenever possible, in an environment mimicking the model‚Äôs deployment. This approach essentially gives up on generalization. Yet, an ML architecture called_Transformer_ has enabled previously unimaginable generalization in other domains. Nowadays, one can download a model pre-trained on massive datasets and only fine-tune it for a specific task and context with comparatively little time and data. These fine-tuned models are now state-of-the-art for many benchmarks. We believe this progress could translate to networking and propose a Network Traffic Transformer (NTT), a transformer adapted to learn network dynamics from packet traces. Our initial results are promising: NTT seems able to generalize to new prediction tasks and contexts. This study suggests there is still hope for generalization, though it calls for a lot of future research.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">dietmuller2022new</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Dietm\"{u}ller, Alexander and Ray, Siddhant and Jacob, Romain and Vanbever, Laurent}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A New Hope for Network Model Generalization}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450398992}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3563766.3564104}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3563766.3564104}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 21st ACM Workshop on Hot Topics in Networks}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{152‚Äì159}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{8}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{packet-level modeling, transformer}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Austin, Texas}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{HotNets '22}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%73%69%64%64%68%61%6E%74.%72%39%38@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fas fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=sHQFKFUAAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/Siddhant-Ray" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/siddhant-ray" title="LinkedIn" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin"></i></a> <a href="https://twitter.com/siddhantrayyy" title="Twitter" target="_blank" rel="noopener noreferrer"><i class="fab fa-twitter"></i></a> </div> <div class="contact-note"> Best way to reach out to me is by e-mail. </div> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> ¬© Copyright 2025 Siddhant Ray. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> based on <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Last updated: September 30, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> </body> </html>